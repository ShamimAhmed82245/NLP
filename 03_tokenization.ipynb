{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i_GXqnTx1U7",
        "outputId": "a36fedee-6fc2-47f0-e2e5-bf2aa75ffe1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aaJVHSyxa31",
        "outputId": "72b12da7-e684-4f16-967f-803c3ac0af1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'this', 'article', ',', 'we', 'are', 'learning', 'word', 'tokenization', 'using', 'NLTK', '.']\n"
          ]
        }
      ],
      "source": [
        "#Word Tokenization\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"In this article, we are learning word tokenization using NLTK.\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Tokenization\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"Hello! Sentence tokenization is essential for breaking down a text into \\\n",
        "its constituent sentences, which is a fundamental step in natural language \\\n",
        "processing. It allows you to work with sentences individually, \\\n",
        "making it easier to perform tasks like sentiment analysis, text summarization,\\\n",
        "and machine translation. NLTK provides a simple way to achieve sentence \\\n",
        "tokenization in Python.\"\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "for sentence in sentences:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrDX-5qryEu2",
        "outputId": "23b31c78-812e-43bf-c59b-58aab4bc4efd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello!\n",
            "Sentence tokenization is essential for breaking down a text into its constituent sentences, which is a fundamental step in natural language processing.\n",
            "It allows you to work with sentences individually, making it easier to perform tasks like sentiment analysis, text summarization,and machine translation.\n",
            "NLTK provides a simple way to achieve sentence tokenization in Python.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Character Tokenization\n",
        "\n",
        "text = \"Hello World!\"\n",
        "\n",
        "characters = list(text)\n",
        "print(\"Character:\",characters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvrMCzG_zKea",
        "outputId": "e919a33b-a90b-4864-8e10-69eba9958507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character: ['H', 'e', 'l', 'l', 'o', ' ', 'W', 'o', 'r', 'l', 'd', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ftT6bxi6zkSp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}