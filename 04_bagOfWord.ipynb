{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL0JOx7D7lkf",
        "outputId": "82d6be1a-11de-4fcb-9aa3-dae67373a8b3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9YG7Rt92-nM",
        "outputId": "aa499203-e97d-4bee-e51f-7dd3eeb8f942"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'really', 'disappointed', 'this', 'product.']\n",
            "----------------------------------------------\n",
            "['I', 'really', 'disappointed', 'product.']\n",
            "['I', 'would', 'not', 'use', 'it', 'again.']\n",
            "----------------------------------------------\n",
            "['I', 'would', 'use', 'again.']\n",
            "['It', 'has', 'really', 'bad', 'feature.']\n",
            "----------------------------------------------\n",
            "['It', 'really', 'bad', 'feature.']\n",
            "['I', 'love', 'this', 'product!']\n",
            "----------------------------------------------\n",
            "['I', 'love', 'product!']\n",
            "['It', 'has', 'some', 'good', 'features']\n",
            "----------------------------------------------\n",
            "['It', 'good', 'feature']\n",
            "['i really disappointed product', 'i would use again', 'it really bad feature', 'i love product', 'it good feature']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def preprocessing_text(text):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  emoji_pattern = r'^(?:[\\u2700-\\u27bf]|(?:\\ud83c[\\udde6-\\uddff]){1,2}|(?:\\ud83d[\\udc00-\\ude4f]){1,2}|[\\ud800-\\udbff][\\udc00-\\udfff]|[\\u0021-\\u002f\\u003a-\\u0040\\u005b-\\u0060\\u007b-\\u007e]|\\u3299|\\u3297|\\u303d|\\u3030|\\u24c2|\\ud83c[\\udd70-\\udd71]|\\ud83c[\\udd7e-\\udd7f]|\\ud83c\\udd8e|\\ud83c[\\udd91-\\udd9a]|\\ud83c[\\udde6-\\uddff]|\\ud83c[\\ude01-\\ude02]|\\ud83c\\ude1a|\\ud83c\\ude2f|\\ud83c[\\ude32-\\ude3a]|\\ud83c[\\ude50-\\ude51]|\\u203c|\\u2049|\\u25aa|\\u25ab|\\u25b6|\\u25c0|\\u25fb|\\u25fc|\\u25fd|\\u25fe|\\u2600|\\u2601|\\u260e|\\u2611|[^\\u0000-\\u007F])+$'\n",
        "\n",
        "  text = text.split()\n",
        "  print(text)\n",
        "  text = [lemmatizer.lemmatize(word) for word in text if not word in set(stopwords.words('english'))]\n",
        "  print(\"----------------------------------------------\")\n",
        "  print(text)\n",
        "  text = ' '.join(text)\n",
        "  text = re.sub(r'[0-9]+', '', text)\n",
        "  text = re.sub(r'[^\\w\\s]', '',text)\n",
        "  text = re.sub(emoji_pattern, r'', text)\n",
        "  text = re.sub(r'\\s+',' ',text)\n",
        "  text = text.lower().strip()\n",
        "  return text\n",
        "paragraph = \"\"\"I am really disappointed this product. I would not use it again. It has really bad feature.\n",
        "I love this product! It has some good features\"\"\"\n",
        "\n",
        "sentences_list = nltk.sent_tokenize(paragraph)\n",
        "\n",
        "corpus = [preprocessing_text(sentence) for sentence in sentences_list]\n",
        "\n",
        "print(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "X_array = X.toarray()\n",
        "\n",
        "print(\"Unique Word List: \\n\", feature_names)\n",
        "print(\"Bag of Words Matrix: \\n\",X_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lQSGpHa7gJR",
        "outputId": "c6310e54-9b6d-42f4-cce4-5196c6c90dec"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Word List: \n",
            " ['again' 'bad' 'disappointed' 'feature' 'good' 'it' 'love' 'product'\n",
            " 'really' 'use' 'would']\n",
            "Bag of Words Matrix: \n",
            " [[0 0 1 0 0 0 0 1 1 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 1 1]\n",
            " [0 1 0 1 0 1 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 1 1 0 0 0]\n",
            " [0 0 0 1 1 1 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(data=X_array, columns=feature_names, index=corpus)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nyv1UzizAvfb",
        "outputId": "faade22a-1533-47a7-c41b-3451fa5710af"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                               again  bad  disappointed  feature  good  it  \\\n",
            "i really disappointed product      0    0             1        0     0   0   \n",
            "i would use again                  1    0             0        0     0   0   \n",
            "it really bad feature              0    1             0        1     0   1   \n",
            "i love product                     0    0             0        0     0   0   \n",
            "it good feature                    0    0             0        1     1   1   \n",
            "\n",
            "                               love  product  really  use  would  \n",
            "i really disappointed product     0        1       1    0      0  \n",
            "i would use again                 0        0       0    1      1  \n",
            "it really bad feature             0        0       1    0      0  \n",
            "i love product                    1        1       0    0      0  \n",
            "it good feature                   0        0       0    0      0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q2wrI8NQBnpG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}